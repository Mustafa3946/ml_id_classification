{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ecfabd6-117b-4c49-8db1-f8d6698192ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import zipfile\n",
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59872c40-fa3f-4a57-a034-c924ba4c6165",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e4a29db-f63e-468e-b376-69b14393f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "s3_bucket = \"id-classifier-images\"\n",
    "s3_key = \"images.zip\"\n",
    "local_zip_path = \"/tmp/images.zip\"\n",
    "dataset_dir = \"/tmp/dataset\"\n",
    "\n",
    "# Download and extract dataset from S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "# s3.download_file(s3_bucket, s3_key, local_zip_path)\n",
    "\n",
    "# with zipfile.ZipFile(local_zip_path, \"r\") as zip_ref:\n",
    "#     zip_ref.extractall(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1cca34-209d-437f-862e-a6e7d165762f",
   "metadata": {},
   "source": [
    "### Initialize SageMaker session and role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61e40626-95f0-412a-96a2-f8fdd948f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fbb95-bd2e-42f0-8a8b-f6f01f41c98a",
   "metadata": {},
   "source": [
    "### Define image parameters for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff64b16a-49a4-4c25-970b-4a7cffd501ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 150  # Height of input images\n",
    "IMG_WIDTH = 150   # Width of input images\n",
    "BATCH_SIZE = 32   # Number of images per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20215a55-8a47-4346-a919-0dd06d578aca",
   "metadata": {},
   "source": [
    "### Create ImageDataGenerator for data augmentation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2376e6a6-3fb4-4449-966a-06d68979047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Normalize pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81d53f-c0d8-46d1-bfe4-73137c451a49",
   "metadata": {},
   "source": [
    "### Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ebdadf-0aad-4bf5-85ad-53776c869505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "unzip_dir = \"/tmp/dataset/images\"\n",
    "train_generator = data_gen.flow_from_directory(\n",
    "    unzip_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use 80% of data for training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d88f4-59df-4fcb-9b2d-2575bc8d238d",
   "metadata": {},
   "source": [
    "### Load validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bfdb94a-ec84-4efa-8c3c-72130d1374d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = data_gen.flow_from_directory(\n",
    "    unzip_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use 20% of data for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e2b86-6072-49a2-b532-8b206561d3f7",
   "metadata": {},
   "source": [
    "### Define a simple Convolutional Neural Network (CNN) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43f389c4-6f7a-42b9-ac04-6cb8155fd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(train_generator.class_indices), activation='softmax')  # Output layer with softmax activation\n",
    "    ])\n",
    "    \n",
    "    # Compile the model with categorical crossentropy loss and Adam optimizer\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d892733-bc60-4d91-be34-b2eda7e08e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
